{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# TORCH\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = pd.read_csv(\"expression1.csv\")\n",
    "yi = pd.read_csv(\"class.csv\")\n",
    "Xi = np.array(Xi)\n",
    "X = np.transpose(Xi)\n",
    "yi = np.array(yi)\n",
    "yi = np.concatenate(yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing dataset using Min Max technique\n",
    "X_min = X.min()\n",
    "X_max = X.max()\n",
    "X_range = (X_max- X_min)\n",
    "X_scaled = (X - X_min)/(X_range)\n",
    "X = X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a 2d matrix for furthur processing\n",
    "def split_sequence(x,n_steps_in):\n",
    "    end_ix = 0\n",
    "    final = []\n",
    "    while(len(x)>end_ix):\n",
    "        X = []\n",
    "        for item in x[end_ix:n_steps_in]:\n",
    "            X.append(item)\n",
    "        end_ix = end_ix + 50\n",
    "        n_steps_in = n_steps_in + 50 \n",
    "        final.append(X)\n",
    "    w=np.asarray(final)\n",
    "    return w\n",
    "x = []\n",
    "n_steps_in = 50\n",
    "\n",
    "for k in X:\n",
    "    X_0 = split_sequence(k, n_steps_in)\n",
    "    x.append(X_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide dataset into 57 samples for training and 5 samples for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, yi, test_size = 0.08)\n",
    "ltrain=len(X_train)\n",
    "ltest=len(X_test)\n",
    "#Reshape the samples for tensor\n",
    "X_train.shape=(ltrain,1,40,50)\n",
    "y_train.shape=(ltrain)\n",
    "X_test.shape=(ltest,1,40,50) \n",
    "y_test.shape=(ltest)\n",
    "# Convert train data into tensors and Wrap them in Variable\n",
    "X_train = torch.Tensor(X_train).float()\n",
    "y_train = torch.Tensor(y_train).long()\n",
    "# Convert test data into tensors and wrap them in Variable\n",
    "X_test = torch.Tensor(X_test).float()\n",
    "y_test = torch.Tensor(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.size())\n",
    "print(X_test.size())\n",
    "print(y_train.size())\n",
    "print(y_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check availability of GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is available!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    '''(conv => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assembly of the sub-parts to form the complete net\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256)\n",
    "        self.up2 = up(512, 128)\n",
    "        self.up3 = up(256, 64)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        xf = x\n",
    "        x = self.outc(x)\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        return x,xf\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size=x.size()[1:]\n",
    "        num_features=1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(n_channels=1, n_classes=2)\n",
    "if use_gpu:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "trainLoss = []\n",
    "testLoss = []\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(iterations):\n",
    "    epochStart = time.time()\n",
    "    runningLoss = 0   \n",
    "    net.train(True) # For training\n",
    "    # Initialize gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Feed-forward input data through the network\n",
    "    out,ft = net(X_train)\n",
    "    out = F.log_softmax(out,dim=1)\n",
    "    loss = criterion(out, y_train)\n",
    "    # Backpropagate loss and compute gradients\n",
    "    loss.backward()\n",
    "    # Update the network parameters\n",
    "    optimizer.step()\n",
    "    # Accumulate loss\n",
    "    runningLoss += loss.item()\n",
    "    avgTrainLoss = runningLoss/ltrain    \n",
    "    trainLoss.append(avgTrainLoss)\n",
    "    \n",
    "    # Evaluating performance on test set for each epoch\n",
    "    net.train(False) # For testing\n",
    "    test_runningLoss = 0    \n",
    "    outputs,q = net(X_test)       \n",
    "    # Compute loss/error\n",
    "    loss = criterion(F.log_softmax(outputs,dim=1), y_test)      \n",
    "    # Accumulate loss per batch\n",
    "    test_runningLoss += loss.item() \n",
    "    avgTestLoss = test_runningLoss/ltest    \n",
    "    testLoss.append(avgTestLoss)\n",
    "        \n",
    "    # Plotting Loss vs Epochs\n",
    "    fig1 = plt.figure(1)        \n",
    "    plt.plot(range(epoch+1),trainLoss,'r--',label='train')        \n",
    "    plt.plot(range(epoch+1),testLoss,'g--',label='test')        \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')   \n",
    "   \n",
    "    epochEnd = time.time()-epochStart\n",
    "    print('At Iteration: {:.0f} /{:.0f}  ;  Training Loss: {:.6f}; Time consumed: {:.0f}m {:.0f}s '\\\n",
    "          .format(epoch + 1,iterations,avgTrainLoss,epochEnd//60,epochEnd%60))\n",
    "    \n",
    "    print('At Iteration: {:.0f} /{:.0f}  ;  Testing Loss: {:.6f} ; Time consumed: {:.0f}m {:.0f}s '\\\n",
    "          .format(epoch + 1,iterations,avgTestLoss,epochEnd//60,epochEnd%60))\n",
    "    \n",
    "end = time.time()-start\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(end//60,end%60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get prediction \n",
    "segImg,g = net(Variable(X_test))\n",
    "# Applying softmax to get class probabilities\n",
    "segImg_np = F.log_softmax(segImg,dim=1).data.cpu().squeeze(1)\n",
    "_, predicted = torch.max(segImg_np, 1) \n",
    "print(predicted)\n",
    "print(y_test)\n",
    "print('Accuracy of the network %d %%' % (100 * torch.sum(y_test==predicted) / ltest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=predicted.data.numpy()\n",
    "cm = np.array(confusion_matrix(ytest, predicted, labels=[1,0]))\n",
    "confusion = pd.DataFrame(cm, index=['is_cancer', 'is_healthy'],\n",
    "                         columns=['predicted_cancer','predicted_healthy'])\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(confusion,annot=True,fmt=\"d\")\n",
    "print(classification_report(y_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f00 = ft[0][0].detach().numpy()\n",
    "np.savetxt(\"f00.csv\",f00)\n",
    "f01 = ft[0][1].detach().numpy()\n",
    "np.savetxt(\"f01.csv\",f01)\n",
    "f02 = ft[0][2].detach().numpy()\n",
    "np.savetxt(\"f02.csv\",f02)\n",
    "f03 = ft[0][3].detach().numpy()\n",
    "np.savetxt(\"f03.csv\",f03)\n",
    "f04 = ft[0][4].detach().numpy()\n",
    "np.savetxt(\"f04.csv\",f04)\n",
    "f05 = ft[0][5].detach().numpy()\n",
    "np.savetxt(\"f05.csv\",f05)\n",
    "f06 = ft[0][6].detach().numpy()\n",
    "np.savetxt(\"f06.csv\",f06)\n",
    "f07 = ft[0][7].detach().numpy()\n",
    "np.savetxt(\"f07.csv\",f07)\n",
    "f08 = ft[0][8].detach().numpy()\n",
    "np.savetxt(\"f08.csv\",f08)\n",
    "f09 = ft[0][9].detach().numpy()\n",
    "np.savetxt(\"f09.csv\",f09)\n",
    "f010 = ft[0][10].detach().numpy()\n",
    "np.savetxt(\"f010.csv\",f010)\n",
    "f011 = ft[0][11].detach().numpy()\n",
    "np.savetxt(\"f011.csv\",f011)\n",
    "f012 = ft[0][12].detach().numpy()\n",
    "np.savetxt(\"f012.csv\",f012)\n",
    "f013 = ft[0][13].detach().numpy()\n",
    "np.savetxt(\"f013.csv\",f013)\n",
    "f014 = ft[0][14].detach().numpy()\n",
    "np.savetxt(\"f014.csv\",f014)\n",
    "f015 = ft[0][15].detach().numpy()\n",
    "np.savetxt(\"f015.csv\",f01)\n",
    "f016 = ft[0][16].detach().numpy()\n",
    "np.savetxt(\"f016.csv\",f016)\n",
    "f017 = ft[0][17].detach().numpy()\n",
    "np.savetxt(\"f017.csv\",f017)\n",
    "f018 = ft[0][18].detach().numpy()\n",
    "np.savetxt(\"f018.csv\",f018)\n",
    "f019 = ft[0][19].detach().numpy()\n",
    "np.savetxt(\"f019.csv\",f019)\n",
    "f020 = ft[0][20].detach().numpy()\n",
    "np.savetxt(\"f020.csv\",f020)\n",
    "f021 = ft[0][21].detach().numpy()\n",
    "np.savetxt(\"f021.csv\",f021)\n",
    "f022 = ft[0][22].detach().numpy()\n",
    "np.savetxt(\"f022.csv\",f022)\n",
    "f023 = ft[0][23].detach().numpy()\n",
    "np.savetxt(\"f023.csv\",f023)\n",
    "f024 = ft[0][24].detach().numpy()\n",
    "np.savetxt(\"f024.csv\",f024)\n",
    "f025 = ft[0][25].detach().numpy()\n",
    "np.savetxt(\"f025.csv\",f025)\n",
    "f026 = ft[0][26].detach().numpy()\n",
    "np.savetxt(\"f026.csv\",f026)\n",
    "f027 = ft[0][27].detach().numpy()\n",
    "np.savetxt(\"f027.csv\",f027)\n",
    "f028 = ft[0][28].detach().numpy()\n",
    "np.savetxt(\"f028.csv\",f028)\n",
    "f029 = ft[0][29].detach().numpy()\n",
    "np.savetxt(\"f029.csv\",f029)\n",
    "f030 = ft[0][30].detach().numpy()\n",
    "np.savetxt(\"f030.csv\",f030)\n",
    "f031 = ft[0][31].detach().numpy()\n",
    "np.savetxt(\"f031.csv\",f031)\n",
    "f032 = ft[0][32].detach().numpy()\n",
    "np.savetxt(\"f032.csv\",f032)\n",
    "f033 = ft[0][33].detach().numpy()\n",
    "np.savetxt(\"f033.csv\",f033)\n",
    "f034 = ft[0][34].detach().numpy()\n",
    "np.savetxt(\"f034.csv\",f034)\n",
    "f035 = ft[0][35].detach().numpy()\n",
    "np.savetxt(\"f035.csv\",f035)\n",
    "f036 = ft[0][36].detach().numpy()\n",
    "np.savetxt(\"f036.csv\",f036)\n",
    "f037 = ft[0][37].detach().numpy()\n",
    "np.savetxt(\"f037.csv\",f037)\n",
    "f038 = ft[0][38].detach().numpy()\n",
    "np.savetxt(\"f038.csv\",f038)\n",
    "f039 = ft[0][39].detach().numpy()\n",
    "np.savetxt(\"f039.csv\",f039)\n",
    "f040 = ft[0][40].detach().numpy()\n",
    "np.savetxt(\"f040.csv\",f040)\n",
    "f041 = ft[0][41].detach().numpy()\n",
    "np.savetxt(\"f041.csv\",f041)\n",
    "f042 = ft[0][42].detach().numpy()\n",
    "np.savetxt(\"f042.csv\",f042)\n",
    "f043 = ft[0][43].detach().numpy()\n",
    "np.savetxt(\"f043.csv\",f043)\n",
    "f044 = ft[0][44].detach().numpy()\n",
    "np.savetxt(\"f044.csv\",f044)\n",
    "f045 = ft[0][45].detach().numpy()\n",
    "np.savetxt(\"f045.csv\",f045)\n",
    "f046 = ft[0][46].detach().numpy()\n",
    "np.savetxt(\"f046.csv\",f046)\n",
    "f047 = ft[0][47].detach().numpy()\n",
    "np.savetxt(\"f047.csv\",f047)\n",
    "f048 = ft[0][48].detach().numpy()\n",
    "np.savetxt(\"f048.csv\",f048)\n",
    "f049 = ft[0][49].detach().numpy()\n",
    "np.savetxt(\"f049.csv\",f049)\n",
    "f050 = ft[0][50].detach().numpy()\n",
    "np.savetxt(\"f050.csv\",f050)\n",
    "f051 = ft[0][51].detach().numpy()\n",
    "np.savetxt(\"f051.csv\",f051)\n",
    "f052 = ft[0][52].detach().numpy()\n",
    "np.savetxt(\"f052.csv\",f052)\n",
    "f053 = ft[0][53].detach().numpy()\n",
    "np.savetxt(\"f053.csv\",f053)\n",
    "f054 = ft[0][54].detach().numpy()\n",
    "np.savetxt(\"f054.csv\",f054)\n",
    "f055 = ft[0][55].detach().numpy()\n",
    "np.savetxt(\"f055.csv\",f055)\n",
    "f056 = ft[0][56].detach().numpy()\n",
    "np.savetxt(\"f056.csv\",f056)\n",
    "f057 = ft[0][57].detach().numpy()\n",
    "np.savetxt(\"f057.csv\",f057)\n",
    "f058 = ft[0][58].detach().numpy()\n",
    "np.savetxt(\"f058.csv\",f058)\n",
    "f059 = ft[0][59].detach().numpy()\n",
    "np.savetxt(\"f059.csv\",f059)\n",
    "f060 = ft[0][60].detach().numpy()\n",
    "np.savetxt(\"f060.csv\",f060)\n",
    "f061 = ft[0][61].detach().numpy()\n",
    "np.savetxt(\"f061.csv\",f061)\n",
    "f062 = ft[0][62].detach().numpy()\n",
    "np.savetxt(\"f062.csv\",f062)\n",
    "f063 = ft[0][63].detach().numpy()\n",
    "np.savetxt(\"f063.csv\",f063)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
